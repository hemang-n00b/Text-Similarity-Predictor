{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-14T03:56:26.643446Z","iopub.status.busy":"2024-02-14T03:56:26.643023Z","iopub.status.idle":"2024-02-14T03:59:16.693497Z","shell.execute_reply":"2024-02-14T03:59:16.691255Z","shell.execute_reply.started":"2024-02-14T03:56:26.643417Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-17 00:21:23.423308: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-02-17 00:21:23.441552: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n","2024-02-17 00:21:23.535981: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-17 00:21:23.536030: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-17 00:21:23.553224: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-02-17 00:21:23.590246: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n","2024-02-17 00:21:23.591739: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-02-17 00:21:24.342864: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/divyansh/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package reuters to /home/divyansh/nltk_data...\n","[nltk_data]   Package reuters is already up-to-date!\n","[nltk_data] Downloading package punkt to /home/divyansh/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /home/divyansh/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package webtext to /home/divyansh/nltk_data...\n","[nltk_data]   Package webtext is already up-to-date!\n"]}],"source":["# !pip install tensorflow --break-system-packages\n","# !pip install nltk --break-system-packages\n","# !pip install numpy --break-system-packages\n","# !pip install pandas --break-system-packages\n","# import tensorflow\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import keras.backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Lambda\n","from keras.utils import to_categorical\n","import numpy as np\n","import nltk\n","import pandas as pd\n","import re\n","\n","# Download the stopwords corpus if not already downloaded\n","nltk.download('stopwords')\n","nltk.download('reuters')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('webtext')\n","from nltk.corpus import stopwords\n","from nltk.corpus import reuters\n","from nltk.corpus import webtext\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def preprocess(content):\n","    \n","    words = word_tokenize(content)\n","    stop_words = set(stopwords.words('english'))\n","    lemmatizer = WordNetLemmatizer()\n","    filtered_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words]\n","    # print(len(filtered_words))\n","    content = ' '.join(filtered_words)\n","    content = re.sub(r\"[^a-zA-Z0-9. ]\", \"\", content)\n","    content = re.sub(r'\\s+', ' ', content)\n","    content = content.lower().strip()     \n","    return content\n","\n","fileids = reuters.fileids()\n","\n","# Initialize empty lists to store categories and raw text\n","categories = []\n","text = []\n","content = \"\"\n","# Loop through each file id and collect each files categories and raw text\n","for file in fileids:\n","    # categories.append(reuters.categories(file))\n","    # text.append(reuters.raw(file))\n","    content = content + reuters.raw(file)\n","\n","content = preprocess(content)\n","# print(len(content))"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def cosine_similarity(vec1, vec2):\n","    dot_product = np.dot(vec1, vec2)\n","    norm_vec1 = np.linalg.norm(vec1)\n","    norm_vec2 = np.linalg.norm(vec2)\n","    similarity = dot_product / (norm_vec1 * norm_vec2)\n","    return similarity\n","\n","def euclidean(vec1,vec2):\n","    return np.linalg.norm(vec1 - vec2)"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocab size is -  7794\n"]}],"source":["##Replace with original corpus\n","#\n","# corpus = [content[:500000]]\n","\n","with open('GloVe/text8', 'r') as f:\n","    content = f.read().lower()\n","\n","# content=content[:100000]\n","# print(content[:1000])\n","sentences=[content.split(\" \")[:50000]]\n","corpus = sentences\n","# print(corpus)\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","sequences = tokenizer.texts_to_sequences(corpus)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","unique_words = list(tokenizer.word_index.keys())\n","# print(unique_words)\n","\n","embed_size=100\n","window_size = 2\n","\n","# print(sequences)\n","\n","print(\"Vocab size is - \", vocab_size)\n","\n","##Generate context,target pairs\n","context=[]\n","target=[]\n","\n","# print(sequences)\n","context.append([0,0,0,0])\n","context.append([0,0,0,0])\n","target.append(sequences[0][0])\n","target.append(sequences[0][1])\n","for i in range(len(sequences)):\n","\n","  for j in range(2,len(sequences[i])-2):\n","    new_cont = []\n","    for k in range(j-2,j+3):\n","      if k!=j:\n","        new_cont.append(sequences[i][k])\n","\n","    context.append(new_cont)\n","    target.append(sequences[i][j])\n","\n","target.append(sequences[0][-2])\n","target.append(sequences[0][-1])\n","context.append([0,0,0,0])\n","context.append([0,0,0,0])\n","\n","# print(len(context),len(target))\n","context = np.array(context)\n","target = np.array(target)\n","\n","# print(context)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 4, 100)            779400    \n","                                                                 \n"," lambda (Lambda)             (None, 100)               0         \n","                                                                 \n"," dense (Dense)               (None, 7794)              787194    \n","                                                                 \n","=================================================================\n","Total params: 1566594 (5.98 MB)\n","Trainable params: 1566594 (5.98 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["2024-02-17 00:22:05.232531: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-02-17 00:22:05.233117: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","Skipping registering GPU devices...\n"]},{"name":"stdout","output_type":"stream","text":["None\n"]}],"source":["##build model\n","\n","cbow = Sequential()\n","cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n","cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n","cbow.add(Dense(vocab_size, activation='softmax'))\n","cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n","\n","print(cbow.summary())\n","\n","one_hot = []\n","\n","for target_val in target:\n","  one_hot.append(to_categorical(target_val,vocab_size))\n","\n","one_hot=np.array(one_hot)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","1563/1563 [==============================] - 5s 3ms/step - loss: 7.9509\n","Epoch 2/100\n","1563/1563 [==============================] - 5s 3ms/step - loss: 7.1196\n","Epoch 3/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.9415\n","Epoch 4/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.8402\n","Epoch 5/100\n","1563/1563 [==============================] - 5s 3ms/step - loss: 6.7611\n","Epoch 6/100\n","1563/1563 [==============================] - 5s 3ms/step - loss: 6.6898\n","Epoch 7/100\n","1563/1563 [==============================] - 5s 3ms/step - loss: 6.6254\n","Epoch 8/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.5634\n","Epoch 9/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.5046\n","Epoch 10/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.4434\n","Epoch 11/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.3852\n","Epoch 12/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.3262\n","Epoch 13/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.2710\n","Epoch 14/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.2162\n","Epoch 15/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.1621\n","Epoch 16/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.1110\n","Epoch 17/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.0613\n","Epoch 18/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 6.0142\n","Epoch 19/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.9657\n","Epoch 20/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.9219\n","Epoch 21/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.8772\n","Epoch 22/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.8331\n","Epoch 23/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.7940\n","Epoch 24/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.7531\n","Epoch 25/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.7133\n","Epoch 26/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.6740\n","Epoch 27/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.6359\n","Epoch 28/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.6026\n","Epoch 29/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.5632\n","Epoch 30/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.5271\n","Epoch 31/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.4939\n","Epoch 32/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.4599\n","Epoch 33/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.4255\n","Epoch 34/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.3910\n","Epoch 35/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.3607\n","Epoch 36/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.3254\n","Epoch 37/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.2957\n","Epoch 38/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.2635\n","Epoch 39/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.2361\n","Epoch 40/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.2050\n","Epoch 41/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.1763\n","Epoch 42/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.1478\n","Epoch 43/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.1217\n","Epoch 44/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.0923\n","Epoch 45/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.0675\n","Epoch 46/100\n","1563/1563 [==============================] - 5s 3ms/step - loss: 5.0409\n","Epoch 47/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 5.0195\n","Epoch 48/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.9928\n","Epoch 49/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.9685\n","Epoch 50/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.9451\n","Epoch 51/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.9234\n","Epoch 52/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.9000\n","Epoch 53/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.8775\n","Epoch 54/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.8568\n","Epoch 55/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.8369\n","Epoch 56/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.8168\n","Epoch 57/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.7946\n","Epoch 58/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.7742\n","Epoch 59/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.7574\n","Epoch 60/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.7365\n","Epoch 61/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.7174\n","Epoch 62/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.6999\n","Epoch 63/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.6841\n","Epoch 64/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.6642\n","Epoch 65/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.6460\n","Epoch 66/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.6276\n","Epoch 67/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.6109\n","Epoch 68/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.5926\n","Epoch 69/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.5765\n","Epoch 70/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.5595\n","Epoch 71/100\n","1563/1563 [==============================] - 5s 3ms/step - loss: 4.5465\n","Epoch 72/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.5272\n","Epoch 73/100\n","1563/1563 [==============================] - 5s 3ms/step - loss: 4.5144\n","Epoch 74/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.4994\n","Epoch 75/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.4850\n","Epoch 76/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.4699\n","Epoch 77/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.4564\n","Epoch 78/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.4411\n","Epoch 79/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.4296\n","Epoch 80/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.4159\n","Epoch 81/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.4025\n","Epoch 82/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.3907\n","Epoch 83/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.3799\n","Epoch 84/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.3687\n","Epoch 85/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.3570\n","Epoch 86/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.3459\n","Epoch 87/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.3352\n","Epoch 88/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.3253\n","Epoch 89/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.3137\n","Epoch 90/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.3050\n","Epoch 91/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.2962\n","Epoch 92/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.2859\n","Epoch 93/100\n","1563/1563 [==============================] - 5s 3ms/step - loss: 4.2768\n","Epoch 94/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.2681\n","Epoch 95/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.2615\n","Epoch 96/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.2508\n","Epoch 97/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.2424\n","Epoch 98/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.2350\n","Epoch 99/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.2246\n","Epoch 100/100\n","1563/1563 [==============================] - 4s 3ms/step - loss: 4.2170\n","[[-5.73850889e-03 -2.46660709e-02  4.33472782e-01 ...  4.18402225e-01\n","   4.14335877e-02  1.07159525e-01]\n"," [-1.13732517e+00  4.87601376e+00 -2.33821362e-01 ... -8.15581322e-01\n","  -1.78404403e+00  8.25701714e+00]\n"," [-1.84881043e+00 -3.50397319e-01 -5.01591086e-01 ... -7.32758105e-01\n","  -2.04164004e+00  8.82225609e+00]\n"," ...\n"," [-1.12011090e-01  2.52785712e-01 -3.91959213e-02 ...  2.58299738e-01\n","   3.76944751e-01  1.19729199e-01]\n"," [-1.65828660e-01  4.71215956e-02  3.17191295e-02 ... -1.82888076e-01\n","  -3.94762784e-01  2.31430814e-01]\n"," [-1.86554603e-02  2.17387348e-01 -7.60664865e-02 ...  1.02327466e-01\n","   1.33622080e-01  2.30451778e-01]]\n"]}],"source":["cbow.fit(context,one_hot,epochs=100,batch_size=32)\n","\n","cbow.save_weights('model_weights.h5')\n","\n","weights = cbow.get_weights()[0]\n","\n","print(weights)\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Similar words for  conflict\n","[('grief', 5.3781104), ('morality', 5.653295), ('blood', 5.663), ('struggle', 5.704911), ('fire', 5.7084312)]\n","Similar words for  electronics\n","[('biochemistry', 1.0752002), ('nutrition', 1.277063), ('vital', 1.3002056), ('vitamin', 1.3947284), ('calendars', 1.5490401)]\n","Similar words for  trouble\n","[('suffered', 5.327976), ('historians', 5.431294), ('originals', 5.727382), ('field', 5.7888293), ('difficulty', 5.790894)]\n","Similar words for  fear\n","[('authorization', 1.8831652), ('byzantine', 2.1199095), ('waldo', 2.2488813), ('campaigns', 2.3174365), ('grateful', 2.3740568)]\n"]}],"source":["\n","words = ['japan','conflict','electronics','rift','trouble','fear']\n","\n","for word in words:\n","    if word in unique_words:\n","        print(\"Similar words for \",word)\n","        word_index = tokenizer.word_index[word]\n","        word_vector = weights[word_index]\n","        similar_words = []\n","        for i in range(1,vocab_size):\n","            if i!=word_index:\n","                similar_words.append((unique_words[i-1],euclidean(word_vector,weights[i])))\n","                \n","        similar_words.sort(key=lambda x: x[1])\n","        print(similar_words[:5])\n","\n","with open(\"vocab.txt\",\"w\") as f2:\n","    for item in unique_words:\n","        f2.write(\"%s\\n\" % item)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
